\section{Background and Motivation}
\label{background}

\begin{figure}
	\centering
	\includegraphics[width=3.4in, height=1.6in]{transformer.pdf}
	\vspace{-0.05in}
	\caption{The LLM model structure.}
	\label{fig:llm}
	\vspace{-0.1in}
\end{figure}

\subsection{Architecture and Workflow of Large Language Models}
\label{sec:llm-basic}


%A generative LLM typically consists of an input layer, tens of consecutive transformer layers, and an output layer, as shown in Figure~\ref{fig:llm}. Assume an input sequence with \( l \) tokens denoted as \( S = [t_0, t_1, \dots, t_{l-1}] \) and an LLM with \( n \) transformer layers. This sequence is first transformed into a tensor \( X_{\text{in}} \) with shape \( l \times d \) by the input layer, where \( d \) is the model’s hidden dimension. \( X_{\text{in}} \) then passes through the first transformer layer, resulting in an intermediate output tensor \( X_{\text{out\_0}} \) that maintains the shape \( l \times d \). This \( X_{\text{out\_0}} \) becomes the input for the subsequent transformer layer. The final block's output, \( X_{\text{out\_(n-1)}} \), is passed to the output layer, generating the first new token \( t_l \). 
%Then, the newly generated token is fed back into the input layer to generate the next token. This process repeats until either the maximum token limit is reached or a special end-of-sequence (EOS) token is generated, signaling the end of the LLM inference process. The generation of each token is referred to as an \textit{iteration}. The process of generating the first token is called the \textit{prefill} phase, while the subsequent token generation is known as the \textit{decoding} phase.

\noindent \textbf{Model inference.}
\fvc{A generative large language model (LLM) consists of an input layer, a stack of transformer layers, and an output layer, as illustrated in Figure~\ref{fig:llm}. Given an input sequence \( S = [t_0, t_1, \dots, t_{l-1}] \) of \( l \) tokens, the input layer maps it to a tensor \( X_{\text{in}} \in \mathbb{R}^{l \times d} \), where \( d \) is the hidden dimension. This tensor is then processed sequentially through \( n \) transformer layers. Each layer must complete its computation before passing its output \( X_{\text{out}_i} \in \mathbb{R}^{l \times d} \) to the next layer, forming a strictly layer-by-layer dependency. The final layer output \( X_{\text{out}_{n-1}} \) is then fed into the output layer to generate the next token \( t_l \). The new token is appended to the input sequence and reprocessed to generate subsequent tokens.This iterative process continues until reaching the maximum token limit or a special end-of-sequence (EOS) token. The generation of the first token is referred to as the \textit{prefill} phase, while the generation of subsequent tokens is termed the \textit{decoding} phase.}


%\textbf{KV Cache Used in Decoding Phase. }
%Since each iteration’s input sequence shares many tokens with the previous iteration, some identical intermediate data (i.e., key and value tensors) are redundantly recalculated, leading to wasted computational resources across iterations.
%To reduce computational overhead, the key and value tensors generated by each transformer layer during an iteration are stored (referred to as the \textit{KV cache}) for reuse in the subsequent iteration. Specifically, in the 0th iteration, the keys and values of all tokens in the input sequence \( S \) must be generated. In subsequent iterations, only the single newly generated token from the last iteration is passed into the LLM, so only that token’s KV needs to be calculated. Due to this difference in computation patterns, researchers often refer to the 0th iteration as the \textit{prefill} phase, with all subsequent iterations collectively known as the \textit{decoding} phase.


%Each transformer layer consists of an attention layer and a feed-forward network (FFN) (we omit the description of layer normalization and residual connections for simplicity in Figure~\ref{fig:llm}). 
%During the prefill phase, the input tensor \( X_{\text{in}} \) is passed through three weight matrices, \( W_q \), \( W_k \), and \( W_v \), to generate three 3D transient tensors: query (Q), key (K), and value (V). Each of these tensors consists of multiple heads (e.g., 3 in Figure~\ref{fig:llm}), with each head containing a 2D tensor referred to as \( q \), \( k \), or \( v \). 
%The Q and K tensors are then used to produce attention weights, where each head has one corresponding 2D attention weight matrix. 
%Each value in the attention weights indicates the relevance of one token to another. 
%The attention weights are then multiplied by the V tensor to form the attention output. 
%This output is passed through a feedforward network (FFN), which consists of two linear layers, ultimately producing the output tensor \( X_{\text{out}} \), with the same shape as \( X_{\text{in}} \).
\noindent \textbf{Transformer layer computation.}
\fvc{Each transformer layer mainly consists of an attention layer and a feed-forward network (FFN), as shown in Figure~\ref{fig:llm}. During the prefill phase, the input tensor \( X_{\text{in}} \) is projected by the weight matrices \( W_q \), \( W_k \), and \( W_v \) to produce the query (\( Q \)), key (\( K \)), and value (\( V \)) tensors. Each tensor consists of multiple attention heads (e.g., three in Figure~\ref{fig:llm}), where each head corresponds to a 2D tensor denoted as \( q \), \( k \), and \( v \), respectively.
The attention weights, computed from \( Q \) and \( K \), represent token relevance and are applied to \( V \) to obtain the attention output. This output then passes through a two-layer FFN to produce \( X_{\text{out}} \), which has the same shape as \( X_{\text{in}} \).}



\begin{figure}
	\centering
	\includegraphics[width=3.4in]{pkvloading.pdf}
	\caption{\cp{The TTFTs of various cases. Assume the LLM model consists of three transformer layers, denoted as `Lx'.}}
	\label{fig:pkvloading}
\end{figure}

\subsection{Shared Prefixes and Storage System}
\label{sec:iobottleneck}
\noindent \textbf{Long TTFT due to the use of context-rich prefixes.}
\fvc{Directly using large models for inference may yield suboptimal results. Specifically, when asked about recent events absent from the training data, the model can produce incorrect or misleading responses due to issues~\cite{siren-arxiv23}. To enhance response quality, applications often augment user \textit{queries} with context-rich \textit{prefixes}, forming complete \textit{requests} that are fed into the LLM as input sequences.
For example, Retrieval-Augmented Generation (RAG)~\cite{rag-nips20} retrieves external documents relevant to the user query. Advanced GPT plugins, such as Chameleon~\cite{chameleon-nips23}, embed tool definitions in the system prompt and use few-shot examples to guide complex reasoning. Multi-turn dialogue systems~\cite{attentionstore-atc24} incorporate previous question–answer pairs to better capture user intent, while self-consistency~\cite{selfcons-ase23} improves accuracy by generating multiple responses and aggregating them through voting.}
%Directly using large models for inference can lead to suboptimal results. For instance, when queried about a recent event not included in the model's training data, the model might provide incorrect answers. Additionally, due to issues such as hallucinations, the model's responses might contain inaccurate or misleading information~\cite{siren-arxiv23}. To improve response quality, applications often prepend user \textit{queries} with additional context-rich \textit{prefixes} to form complete \textit{requests}, which are then fed into the LLM as input sequences.
%For example, Retrieval-Augmented Generation (RAG)~\cite{rag-nips20} searches external knowledge bases for documents relevant to the user's query. Advanced GPT plugins, such as Chameleon~\cite{chameleon-nips23}, include tool definitions in the system prompt and use few-shot examples to guide the LLM in performing complex reasoning tasks. Multi-turn dialogue applications~\cite{attentionstore-atc24} add previous question-answer pairs to the user's latest query for better intent understanding, while the self-consistency technique~\cite{selfcons-ase23} generates multiple responses to the same query and uses voting to improve accuracy.

\fvc{Figures \ref{fig:pkvloading}(a) and \ref{fig:pkvloading}(b) show that although context-rich prefixes enhance response quality, they substantially increase the delay before the first token is produced, known as the time to first token (TTFT). For example, Chameleon adds more than 2,600 tokens before each user query \cite{chameleon-prompt}. Given that an average real-world query contains approximately 750 tokens \cite{sharegpt}, this quadruples the input length and increases TTFT by up to nine times for the OPT-30B model. Such latency severely affects user experience in TTFT-sensitive applications such as real-time chatbots and also reduces system throughput, leading to higher operational costs.}
%Figure~\ref{fig:pkvloading}(a) and Figure~\ref{fig:pkvloading}(b) show that while these context-rich prefixes improve the quality of responses, they also significantly increase the time-to-first-token (TTFT), which is the delay before the model generates the first token. For instance, the Chameleon system adds over 2,600 tokens of context before the user's query~\cite{chameleon-prompt}. Given that the average real-world user query is about 750 tokens~\cite{sharegpt}, this increases the request token count by more than 4$\times$, extending the TTFT by 9$\times$ for the OPT-30B model due to the additional computation. This can negatively impact user experience, especially in TTFT-sensitive applications like real-time chatbots. 
%Besides, it also degrades the system's overall throughput and increases the enterprise costs.	
%This paper focuses on reducing TTFT during the prefill phase without altering the decoding phase. Note that shortening TTFT also reduces decoding latency for other requests, as modern systems use continuous batching, where the decoding of existing and newly-arrived requests are processed together after the completion of the prefill of the new requests~\cite{orca-osdi22}.

\noindent \textbf{Prefix KV storage systems and I/O bottleneck.}
Researchers have observed that these prefixes are often partially or completely shared across different requests~\cite{sglang-arxiv23, chunkattention-arxiv24, cachegen-sigcomm24, ragcache-arxiv24, promptcache-mlsys24, attentionstore-atc24}. For example, similar queries might retrieve partially or entirely the same related documents using RAG; the same GPT plugin can be used multiple times, resulting in identical system prompts across requests. 
Recomputing the K and V tensors in Figure~\ref{fig:llm} for the same prefix leads to wasted computational resources and increased TTFT. To optimize TTFT, existing systems store and reuse the K and V tensors of these shared prefixes (referred to as \textit{prefix KV cache} or simply \textit{prefix KV}). Note that the Q tensor of the prefix is not stored, as it is not needed for subsequent computations~\cite{attentionstore-atc24}. When a new request with a repeated prefix arrives, the system asynchronously preloads its prefix KVs into GPU memory, thereby reducing TTFT during the prefill phase of the new request.
\begin{figure}
	\centering
	\includegraphics[width=3.3in, height=1.55in]{pload_bottleneck.pdf}
	\vspace{-0.1in}
	\caption{TTFT breakdown.
				`ReComp' refers to not reusing the prefix KV. `QueryComp' denotes the remaining computation after loading the prefix KV. }
%	\vspace{-0.1in}
	\label{fig:pload-bottleneck}
	\vspace{-0.15in}
\end{figure}
\cp{\subsection{Limitations of Existing Prefix KV Storage Systems}}
\begin{table}[t]
	\centering
	\caption{\cp{Comparison of prefix KV storage designs.}}
	\label{tab:pkv_comparison}
	\setlength{\tabcolsep}{3pt}  
	\renewcommand{\arraystretch}{1.15}
		\begin{tabular}{lccccc}
			\specialrule{1.2pt}{0pt}{2pt}
			\textbf{System} & \makecell{\textbf{CPU}\\\textbf{Memory}} & \textbf{Disk} & \makecell{\textbf{Import.}\\\textbf{Aware}} & \textbf{Caching} & \textbf{Prefetching} \\
			\specialrule{1.2pt}{0pt}{2pt}
			\textit{PromptCache}~\cite{promptcache-mlsys24} & \cmark & \xmark & \xmark & \cmark & \cmark \\
			\textit{SGLang}~\cite{sglang-arxiv23}           & \cmark & \xmark & \xmark & \cmark & \cmark \\
			\textit{RAGCache}~\cite{ragcache-arxiv24}       & \cmark & \xmark & \xmark & \cmark & \cmark \\
			\textit{ChunkAttention}~\cite{chunkattention-arxiv24} & \cmark & \xmark & \xmark & \xmark & \cmark \\
			\textit{AttentionStore}~\cite{attentionstore-atc24}   & \cmark & \cmark & \xmark & \cmark & \cmark \\
			\textit{IMPRESS}~\cite{impress-fast25}          & \cmark & \cmark & \cmark & \cmark & \xmark \\
			\textit{\pname{} (Ours)}          & \cmark & \cmark & \cmark & \cmark & \cmark \\
			\specialrule{1.2pt}{0pt}{2pt}
		\end{tabular}
\end{table}


\cp{An effective prefix KV storage system must satisfy two key requirements. First, it must provide sufficient storage capacity to hold a large number of prefix KVs. Second, it must ensure low latency for prefetching prefix KVs, as excessive loading delay can become a bottleneck in LLM inference and limit the reduction of time-to-first-token (TTFT).
To achieve these goals, techniques such as caching, prefetching, and importance-aware loading  
are often employed to reduce I/O overhead and improve end-to-end efficiency. 
However, as summarized in Table~\ref{tab:pkv_comparison}, 
existing systems optimize only a subset of these dimensions, 
and none achieves a comprehensive solution across all aspects.}

\cp{\paragraph{KV stores using only GPU/CPU memory.}  
Systems such as PromptCache~\cite{promptcache-mlsys24}, 
SGLang~\cite{sglang-arxiv23}, 
RAGCache~\cite{ragcache-arxiv24}, 
and ChunkAttention~\cite{chunkattention-arxiv24} 
store prefix KVs solely in GPU and/or CPU memory to minimize prefetch latency, 
as illustrated in Figure~\ref{fig:pkvloading}(c). 
This design achieves low TTFT by avoiding I/O operations and typically employs caching and prefetching 
to further reduce latency. 
However, GPU and CPU memory are inherently capacity-limited, 
causing these systems to quickly exhaust available storage in RAG workloads. 
For example, with OPT-30B in FP16, the KV cache is 1.31 MB per token; an 8,000-token retrieved prefix consumes 11 GB, and eight concurrent sessions require 88 GB which exceeds a single 80 GB GPU like A100.}

\cp{\paragraph{KV stores using CPU memory and disks.}  
To overcome the capacity constraint, 
AttentionStore~\cite{attentionstore-atc24} stores prefix KVs across both CPU memory and disks, 
providing sufficient capacity for large-context inference. 
However, reading data from disk is inherently slow. 
To mitigate this issue, AttentionStore leverages layer-level parallelism by prefetching the KVs 
for the next Transformer layer while the current layer is being computed, as shown in Figure~\ref{fig:pkvloading}(d). 
Nevertheless, the data transfer from SSD to GPU remains slow, 
and the latency cannot be fully hidden by computation, 
especially under heavy or preemptive workloads. As shown in Figure~\ref{fig:pload-bottleneck}, 
loading prefix KVs from SSD still leads to a substantial increase in TTFT, 
as I/O latency accounts for 51\% to 98\% of the total and rarely overlaps completely with query computation.}

	

\cp{\paragraph{Importance-aware KV store.}  
To mitigate the I/O bottleneck caused by loading prefix KVs from disk, IMPRESS~\cite{impress-fast25} introduces a novel approach that loads only the most important KVs to reduce I/O latency while maintaining comparable model accuracy. However, these selected KVs still lie on the critical path of model inference, as shown in Figure~\ref{fig:pkvloading}(e).
Although prior systems employ prefetching to overlap data loading with model computation by fetching all KVs in advance, 
this strategy fails in IMPRESS. 
Since IMPRESS loads only the most important KVs, 
the selection of these KVs for the next layer depends on the computation results of the current layer. 
As a result, the system cannot determine in advance which KVs to prefetch, and their I/O latency remains exposed on the critical path, limiting end-to-end inference efficiency.}










%An effective prefix KV storage system must meet two requirements. First, it needs sufficient storage capacity to hold enough prefix KVs. Second, the latency for prefetching the prefix KV must be low. Otherwise, it will become a bottleneck of the LLM inference, limiting the reduction of TTFT. 
%Currently, no system can simultaneously meet these two requirements across various scenarios.
%Most existing systems store prefix KVs only in GPU and/or CPU
%memory~\cite{promptcache-mlsys24, sglang-arxiv23, ragcache-arxiv24,
%chunkattention-arxiv24}, as shown in Figure~\ref{fig:pkvloading}(c), to reduce
%TTFT. However, the limited space in GPU and CPU memory quickly becomes
%exhausted. Although the latest prefix KV storage system,
%AttentionStore~\cite{attentionstore-atc24}, stores the prefix KV on both CPU
%memory and disk to provide sufficient storage space, it doesn’t fundamentally
%reduce the load time, as the latency from disk cannot be fully hidden by
%computation in some cases as shown in Figure~\ref{fig:pkvloading}(d).  
%Thus, this approach may fail under heavy request loads or in preemptive scheduling scenarios, due to the bottleneck of disk I/Os.
%\zrdnew{IMPRESS~\cite{impress-fast25} proposes that loading only important KVs to reduce I/O latency while maintaining comparable accuracy. However, the identification and loading of important KVs for the next layer depend on the computation results of the current layer, which makes it impossible to prefetch them during the current layer's computation. As xxx shows, although this approach reduces the I/O volume, it fails to leverage the opportunity to parallelize I/Os with computation, resulting in the I/O bandwidth being underutilized during computation.}
%Figure~\ref{fig:pload-bottleneck} shows the TTFT breakdown for recalculating
%shared prefixes versus loading and reusing prefix KVs from different storage
%media (i.e., GPU memory, CPU memory, and SSD). We vary the number of input prefix tokens from
%128 to 8k. The ``load''  time represents the I/O latency that cannot be hidden by
%query computation. 
%It shows that loading prefix KVs from GPU or CPU results in shorter TTFT compared to recomputation, but loading from SSD leads to longer TTFT. This is due to the I/O latency from SSD to GPU, which is rarely hidden by query computation and accounts for 51\%-98\% of the total TTFT.
%Consequently, prefix KV loading has become a new bottleneck in model inference, particularly for longer prefixes.



\subsection{Not All KVs Are Equally Important}
Recent research~\cite{h2o-nips23, infinigen-osdi24, flexgen-icml23, scissorhands-nips23} indicates that not all tokens' KVs are equally important for maintaining LLM inference accuracy. These methods generate and store the full set of KVs during the prefill phase, then  identify and discard the less important tokens' KVs during decoding by analyzing the attention weights. This approach reduces the computational load during the decoding phase while maintaining comparable LLM inference accuracy.

\cp{Inspired by this, we propose an importance-informed three-tiered prefix KV caching and prefetching system that encompasses GPU memory, CPU memory, and disk storage. 
Our system addresses the disk I/O bottleneck through a three-pronged approach: selective loading, prefetching, and caching. 
When reusing prefix KVs, we selectively load only important KVs for prefill computations, while discarding those associated with unimportant tokens. 
To further improve I/O efficiency, we introduce the \technew{} mechanism, which leverages the current layer's important token indices to predict and prefetch the next layer's important KVs during computation, effectively overlapping I/O operations with GPU execution. 
Meanwhile, frequently reused KVs are retained in the CPU and GPU cache hierarchy to avoid redundant disk reads in subsequent decoding steps. 
This integrated design of selective loading, cross-layer prefetching, and hierarchical caching fundamentally alleviates the disk bottleneck by reducing the total I/O volume and shifting critical I/O operations off the inference path, thereby significantly reducing TTFT.}
%When
%reusing prefix KVs, we aim to load only important KVs
%%  rather than fetching them
%% all. 
%% \wj{Only important prefix tokens' KVs participate in prefill and decoding
	%% computations, while unimportant tokens' KVs are discarded.}
%for prefill and decoding computations, while unimportant tokens' KVs are discarded.
%Selectively loading the important KVs can
%fundamentally alleviate the disk
%bottleneck, thereby reducing TTFT.

% when reusing prefix KVs 

