\subsection{Importance-Informed KV Placement}
\label{sec:techb}

\subsubsection{\techBa{}}
\label{sec:techba}
% We propose the \techba{} technique to address the issue where unimportant KVs within the same chunk are also loaded and cached when retrieving important KVs. The core idea is to increase the density of important KVs within a chunk by reordering and repacking them into new chunks, thereby reducing bandwidth waste during reads. 
% This reordering and repacking process is performed periodically \wj{(e.g., 10 minutes in our settings)}, based on the average importance of each token throughout the cycle.
% The entire process is executed asynchronously, so the time spent does not impact the main I/O path.

% We introduce the \techba{} method to tackle the problem of loading and caching unimportant KVs alongside important ones within the same chunk. The essence is to enhance the concentration of important KVs in a chunk by periodically reordering and consolidating them into new chunks, which minimizes bandwidth wastage during read operations.
% This reorganization and repacking is scheduled at regular intervals (e.g., every 10 minutes in our configuration), aligned with the average importance of each token over the cycle. The entire operation is conducted asynchronously, ensuring that the time invested does not interfere with the primary I/O operations.

%原文%We present the \techba{} method to address the unnecessary loading of unimportant KVs during important KV retrieval. By periodically reorganizing and repacking important KVs into denser chunks, this approach optimizes read efficiency and reduces bandwidth waste.
%Scheduled at regular intervals (e.g., every 10 minutes), this process is based on the average token importance and operates asynchronously to avoid disrupting the main I/O flow.
\cp{We present the \techba{} method to address the unnecessary loading of unimportant KVs during important KV retrieval. By periodically reorganizing and repacking important KVs into denser chunks, this approach optimizes read efficiency and reduces bandwidth waste.
	Scheduled at regular intervals (e.g., every 10 minutes), this process is based on the average token importance and operates asynchronously to avoid disrupting the main I/O flow.}

\begin{figure}
	\centering
	\includegraphics[width=3.4in, height=1.2in]{reorder.pdf}
%	\vspace{-0.1in}
	\caption{Comparison of the number of chunks read before and after the token sequence is reordered. The orange (blue) rectangles represent important (unimportant) keys.}
	\label{fig:reordering}
	\vspace{-0.1in}
\end{figure}

%To illustrate this process, consider an example where a prefix consists of four tokens [t0, t1, t2, t3], and the existing system stores keys for two adjacent tokens in a single chunk. Suppose each chunk contains one important key and one unimportant key.
%Figure~\ref{fig:reordering}(a) demonstrates that, without the \techba{} technique, both chunks must be loaded from disk into CPU memory to retrieve the two important keys \{t0, t3\}. This leads to unimportant keys consuming valuable read bandwidth and cache space. In contrast, with \techba{} enabled (as shown in Figure~\ref{fig:reordering}(b)), the four tokens are first reordered in a descending order of importance, and then the keys of two adjacent reordered tokens (i.e., [t0, t3] and [t1, t2]) are packed into a single chunk. This allows only one chunk to be loaded to access all two important keys, thereby reducing the amount of data read from the disk.
%原文\fvc{
%To illustrate, consider a prefix [t0, t1, t2, t3] where the existing system stores keys for two adjacent tokens in one chunk, each containing one important and one unimportant key. Without \techba{} (Figure~\ref{fig:reordering}(a)), both chunks must be loaded to retrieve the important keys \{t0, t3\}, wasting read bandwidth and cache space on unimportant keys. With \techba{} (Figure~\ref{fig:reordering}(b)), tokens are reordered by importance, and keys of adjacent reordered tokens ([t0, t3] and [t1, t2]) are packed into one chunk. This enables loading just one chunk to access all important keys, reducing disk read data.
%}
\cp{
	To illustrate, consider a prefix $[t_0, t_1, t_2, t_3, t_4, t_5]$ where the existing system stores keys for two adjacent tokens in one chunk, and each chunk mixes one high-importance and one unimportance key. Assume the importance ranking among the important keys is $t_0 > t_4 > t_3$.
	When loading,
	without reordering (Figure~\ref{fig:reordering}\,(a)), all chunks must be loaded to retrieve the important keys $\{t_0, t_3, t_4\}$, wasting read bandwidth and cache space on unimportance keys. }
	
	\cp{
	With \techba{} as in IMPRESS (Figure~\ref{fig:reordering}(b)), tokens are reordered by importance but packed into chunks according to token IDs (e.g., $[t_0, t_3]$, $[t_1, t_2]$, $[t_4, t_5]$). 
	This organization reduces redundant reads but limits prefetch efficiency when I/O time is constrained. 
	For example, if the prefetching time window allows prefetching only one SSD chunk, the ideal prefetching order based on importance would retrieve $t_0$ and  $t_4$. 
	However, under the data layout of IMPRESS, $t_0$ and $t_4$ are stored in different chunks, while $t_0$ and $t_3$ share one. 
	As a result, since $t_0$ and $t_3$ share the same chunk, fetching the most important key $t_0$ also forces $t_3$ to be loaded together, which prevents $t_4$ from being prefetched within the time limit.
	In contrast, \pname{} (Figure~\ref{fig:reordering}(c)) organizes chunks by importance ranking (e.g., $[t_0, t_4]$, $[t_2, t_3]$, $[t_1, t_5]$), allowing the prefetch of $\{t_0, t_4\}$ under the same budget and thereby maximizing prefetch efficiency.
}


%\noindent \textbf{Metadata adjustment.}
%%In LLM inference, a token's KV can only be reused when that token and all preceding tokens are identical across requests. 
%In LLMs, prefix KVs can only be reused if two prefixes share a common subsequence starting from the first token 
%(i.e., the token order must be the same).
%Therefore, existing systems typically use a radix tree~\cite{sglang-arxiv23, chunkattention-arxiv24} or its variants~\cite{ragcache-arxiv24} to record stored prefix tokens, enabling quick search for reusable stored prefix KVs when a new request arrives. 
%%However, the \techba{} could disrupt the original metadata organization, affecting the ability to check and reuse shared prefix KVs for new requests.
%However, \techba{} may destroy the radix tree structure by altering the token order, 
%causing new requests to fail in locating the correct shared prefix KVs.
%To overcome this, we limit the scope of KV reordering to the tokens within each node of the radix tree. Additionally, we introduce a mapping list within each node to assist the checking operations of new requests.
%We use an example to explain the details.
%
%\begin{figure}
%	\centering
%	\includegraphics[width=3.3in, height=1.2in]{metaupdate.pdf}
%%	\vspace{-0.1in}
%	\caption{Comparison of meta structure before and after KV reordering. The orange (blue) rectangles represent important (unimportant) keys.}
%	\label{fig:metaupdate}
%	\vspace{-0.1in}
%\end{figure}
%
%Consider two requests that retrieve related document segments as prefixes
%through RAG. Each prefix contains eight tokens: p0=[t0, t1, t2, t3, t4, t5, t6,
%t7] for one request and p1=[t0, t1, t2, t3, t8, t9, t10, t11] for the other. t0,
%t3, t4, and t9 are important data, while the remaining tokens are unimportant.
%Before applying the \techba{} technique, the radix tree is organized as shown in
%Figure~\ref{fig:metaupdate}(a), where the common prefix subsequence s0=[t0, t1,
%t2, t3] is grouped within the same node, enabling the reuse of as many prefix
%KVs as possible. Assume the chunk size is set to 2, with each chunk containing the keys or values of two consecutive tokens.
% Each node contains a list of pointers k\_ptr (v\_ptr) to
%these key (value) chunks.
%
%After enabling \techba{}, as shown in Figure~\ref{fig:metaupdate}(b), the token sequence within each node is reordered in a descending order of importance, and the keys and values are repacked. In Node 0, for example, the token sequence becomes s0' = [t0, t3, t1, t2], with t0 and t3 now grouped within the same chunk. 
%Consequently, reordering disrupts the token sequence in the radix tree.
%We add a new mapping list to Node 0 to address this issue.
%It is denoted as m0 = [0, 2, 3, 1], allowing the original s0 sequence to be recovered using the torch index operation s0'[m0] when search reusable shared prefixes for new requests. This vectorized indexing operation is highly efficient, consuming less than 2\% of the TTFT in our experiments. 
%
%We explicitly avoid cross-node reordering, such as placing t4 and t9 into s0', for two key reasons. First, it would destroy the radix tree structure since t4 and t9 are not common tokens shared by both prefixes (i.e., p0 and p1), potentially leading to errors in retrieving reusable prefix segments for new requests. Second, it would result in unnecessary read bandwidth consumption by loading t9 when reusing the KVs of p0. The constraint against cross-node reordering prevents packing unshared tokens' KVs from different prefixes together, thereby reducing bandwidth wastage.


\subsubsection{\techBb{}}
\label{sec:techbb}
To cut down on PCIe transfers, after loading a chunk from disk into CPU
memory, only the important key and value vectors from that chunk are sent to the
GPU memory via PCIe. However, as depicted in Figure~\ref{fig:imp_token_num}, the
presence of important key or value vectors in a chunk does not align with the
chunk's access frequency. Consequently, existing systems that base their caching
decisions for GPU or CPU memory solely on recency or frequency of chunk access
may lower the GPU cache hit ratio for important key-value pairs, thus increasing
PCIe traffic.

\noindent \textbf{Score-based cache admission.}
To enhance cache efficiency, we introduce an importance-aware cache admission policy that assigns each chunk a score combining its access frequency and the proportion of important keys or values it contains.
Chunks with higher scores are cached in GPU memory, while lower-scored ones stay in CPU memory, improving the GPU cache hit ratio and reducing CPU–GPU data transfers.
The importance ratio is updated online using a moving average after each access, allowing dynamic adaptation to workload changes.
Since chunks often contain both important and less important tokens, this scoring balances access frequency and data value to prioritize high-impact chunks.

For example, consider two key chunks: chunk 1 from application A and chunk 2 from application B.
Application A’s request frequency is 1.5× higher than B’s, but chunk 1 holds only one important key (50\%), while chunk 2 holds two (100\%).
As shown in Figure \ref{fig:score_cache}(a), traditional caching stores chunk 1 in GPU memory based on frequency, requiring 20 important keys to be transferred.
By contrast, our policy computes scores of 0.75 (1.5 × 50\%) for chunk 1 and 1.0 (1 × 100\%) for chunk 2, so chunk 2 is cached in GPU memory (Figure \ref{fig:score_cache}(b)), reducing transfers to 15 and improving overall cache efficiency.


\begin{figure}
	\centering
	\includegraphics[width=3.3in, height=1.2in]{score_cache.pdf}
	% \vspace{-0.1in}
	\caption{Comparison of two cache replacement policies.(a) is the
	frequency-based cache replacement policy and (b) is the \techbb{} policy. The orange (blue) rectangles
	represent important (unimportant) keys. Only important keys are needed for
	new requests.}
	\label{fig:score_cache}
	\vspace{-0.1in}
\end{figure}


\begin{figure*}
	\subfigure[PIQA]{
		\includegraphics[width=3.3in, height=1.1in]{overall_acc1_piqa.pdf}
	}
	\hspace{0.1in}
	\subfigure[RTE]{
		\includegraphics[width=3.3in, height=1.1in]{overall_acc1_rte.pdf}
	}
	\subfigure[COPA]{
		\includegraphics[width=3.3in, height=1.1in]{overall_acc1_copa.pdf}
	}
	\hspace{0.2in}
	\subfigure[OpenBookQA]{
		\includegraphics[width=3.3in, height=1.1in]{overall_acc1_openbookqa.pdf}
	}
	
	\vspace{-0.1in}
	\caption{
		Model generation quality of various systems across four datasets and three models.}
	\label{fig:overall_acc}
	\vspace{-0.1in}
\end{figure*}

%\noindent \textbf{Dual-cache replacement algorithm.} 
%The \pname{} system includes both GPU and CPU caches, each managed by different cache eviction strategies due to the varying granularity of data transfers from disk and CPU. Specifically, to optimize disk I/O efficiency, data is transferred from disk to CPU cache in chunks, regardless of the ratio of important KVs within each chunk. Consequently, the CPU cache eviction is only based on chunk access frequency to minimize the number of chunks loaded from the disk. 
%In contrast, when transferring data from the CPU cache to the GPU cache, only important KVs are transmitted to reduce  \ysl{the pressure on the PCIe bus}~\cite{flexgen-icml23}. Therefore, the \techbb{} method is employed in GPU cache eviction to minimize the transmission of important KVs.
%
%To achieve this, 
%we maintain two separate min-heaps in the GPU and CPU caches to assist with cache eviction. The top elements in the GPU heap are chunks with the lowest scores, while in the CPU heap, they are chunks with the lowest access frequency.
%When a new request arrives, the system first checks the radix tree-based meta data to identify reusable stored prefix KV chunks and locate whether they are located in the GPU cache, CPU cache, or disk.
%If the target chunk is in the GPU cache, after the chunk is accessed and its score is updated, it remains in the GPU cache.
%If the target chunk is in the CPU cache, its score is updated and compared with the smallest score in the GPU cache. If the target chunk's score is higher, it is swapped with the lowest-score chunk in the GPU cache; otherwise, it stays in the CPU cache.
%If the target chunk is on the disk, its access frequency is updated and compared with the lowest access frequency in the CPU cache. If the target chunk has a higher access frequency, it replaces the least-accessed chunk in the CPU; otherwise, it stays in the disk.

\noindent \textbf{Dual-cache replacement algorithm.}
% We use \techbb{} to manage both GPU and CPU caches. To achieve this, we maintain two min-heaps in CPU memory for the chunks in GPU and CPU memory to assist with cache eviction. 
% The top elements in two heaps point to the chunks with the lowest scores in the GPU and CPU caches respectively. 
% %We ensure that chunks in the GPU and CPU caches are non-redundant to maximize the amount of cached data. Additionally, 
% We ensure that all chunks' replicas are kept on disk to avoid I/O latency when evicting chunks from the CPU to disk.
We employ a score-based cache replacement policy to coordinate GPU and CPU caches. Two min-heaps in CPU memory track chunks in each cache, with the top entries representing the lowest-scored chunks eligible for eviction. To avoid redundancy, each chunk resides in only one cache, while all replicas are preserved on disk to eliminate I/O latency during eviction.

When a new request arrives, \pname{} locates chunks containing reusable important tokens. If a chunk is already in the GPU cache, its key/value vectors are used directly and its score is updated. If it resides in the CPU cache, the vectors are transferred to the GPU, and the chunk’s score is compared with the lowest in the GPU cache—replacing it only if higher. For chunks on disk, \pname{} loads them into CPU memory, fetches the required vectors, and evaluates their updated scores to determine whether they should be promoted to GPU memory, remain in CPU cache, or stay on disk.


%\subsubsection{\techBb{}}
%当要读取的重要token的key or value vector在disk上时，\pname{} 首先会从disk加载其所在的chunk到CPU中，然后读取仅传送重要的key or value vectors 到GPU中从而减小PCIe传输量，最后决定该chunk是否被GPU或CPU缓存以备后续使用。
%However, we observed that the number of important key or value vectors in a chunk does not correlate with the chunk’s access frequency, as shown in Figure~\ref{fig:imp_token_num}. Therefore, existing systems that determine whether to cache a chunk in GPU memory or CPU memory based solely on access recency or frequency may reduce the hit ratio of important key-value pairs in the GPU cache, leading to an increase in data transferred via PCIe.
%
%To address this, we propose an importance-informed \techbb{} that further improves the GPU cache hit ratio and reduce the data transfer volume from CPU to GPU.
%The core idea is to assign a score to each chunk and use the score to determine whether it should be cached. 
%This score is defined as the cumulative number of important vectors accessed within each chunk.
%as its access frequency multiplied by the ratio of important keys or values it contains. 
%The chunks with higher scores are prioritized for caching in GPU memory, while those with lower scores are placed in CPU memory. 
%The score is updated after each chunk access.
% The ratio of important keys or values is computed as the moving average between two consecutive accesses and updated online after each chunk access.
%
%假设有三个chunk的访问频率分别是8, 10, 7；其中重要的token的数量分别是3, 1, 2。假设GPU和CPU都只能存储1个chunk，新的一些请求分别要访问8、10、7次三个chunk以获取其中重要的token用于推理。
%传统方法使用LFU缓存时会根据chunk访问频率从高到底放置如图x。那么传统方法一共需要经过PCIe传输8*3 + 7*2 = 38个vector。
%使用\techbb{}后，由于chunk1的score大于chunk2的score，因此放置如图x；那么一共需要经过PCIe传输10*1 + 7*2 = 24个vector。
