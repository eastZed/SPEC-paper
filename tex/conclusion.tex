\section{Conclusion}
\label{conclusion}
Existing prefix KV reuse systems do not always reduce TTFT, especially when disk
I/O latency is involved in large-scale LLM services. We propose \pname{}, a
multi-tier prefix KV storage system to 
minimize I/O delay by only loading important KVs. Simply applying existing important token identification
algorithms is suboptimal, as the reduction in I/O is limited. Therefore, we
first introduce the I/O-efficient \techa{} algorithm to identify important KVs
with minimal I/O. Then, we propose \techb{} to optimize storage and caching,
further reducing TTFT. Our experiments show that \pname{} reduces TTFT by up to
2.8$\times$ compared to state-of-the-art systems, while maintaining comparable
inference accuracy.

\section*{Acknowledgments}
\fv{
We sincerely thank the anonymous reviewers for their constructive suggestions. 
This work was supported in part by the National Key Research and Development Program of China (2023YFB4502100), 
the National Science Foundation of China (62172361), 
the Major Projects of Zhejiang Province (LD24F020012), 
the Open Project Program of Wuhan National Laboratory for Optoelectronics (2023WNLOKF005),  
and the Pioneer and Leading Goose R\&D Program of Zhejiang Province (2024SSYS0002).
}