%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Shuibing He at 2024-09-16 07:45:01 +0800 


%% Saved with string encoding Unicode (UTF-8) 



@article{chunkattention-arxiv24,
	author = {Ye, Lu and Tao, Ze and Huang, Yong and Li, Yang},
	date-modified = {2024-09-16 07:29:44 +0800},
	journal = {arXiv preprint arXiv:2402.15220},
	title = {{Chunkattention: Efficient Self-Attention With Prefix-Aware KV Cache and Two-Phase Partition}},
	year = {2024}}

@article{hydragen-arxiv24,
	author = {Juravsky, Jordan and Brown, Bradley and Ehrlich, Ryan and Fu, Daniel Y and R{\'e}, Christopher and Mirhoseini, Azalia},
	bdsk-color = {1},
	date-modified = {2024-09-16 07:14:00 +0800},
	journal = {arXiv preprint arXiv:2402.05099},
	title = {{Hydragen: High-Throughput LLM Inference with Shared Prefixes}},
	year = {2024}}

@article{promptcache-mlsys24,
	author = {Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin},
	journal = {Proceedings of Machine Learning and Systems},
	pages = {325--338},
	title = {{Prompt Cache: Modular Attention Reuse for Low-Latency Inference}},
	volume = {6},
	year = {2024}}

@article{sglang-arxiv23,
	author = {Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Huang, Jeff and Sun, Chuyue and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
	journal = {arXiv preprint arXiv:2312.07104},
	title = {{SGLang: Efficient Execution of Structured Language Model Programs}},
	year = {2023}}

@article{attentionstore-atc24,
	author = {Gao, Bin and He, Zhuomin and Sharma, Puru and Kang, Qingxuan and Jevdjic, Djordje and Deng, Junbo and Yang, Xingkun and Yu, Zhou and Zuo, Pengfei},
	journal = {arXiv preprint arXiv:2403.19708},
	title = {{AttentionStore: Cost-Effective Attention Reuse across Multi-Turn Conversations in Large Language Model Serving}},
	year = {2024}}

@article{ragcache-arxiv24,
	author = {Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin},
	journal = {arXiv preprint arXiv:2404.12457},
	title = {{RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation}},
	year = {2024}}

@inproceedings{cachegen-sigcomm24,
	author = {Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
	booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
	pages = {38--56},
	title = {{CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving}},
	year = {2024}}

@article{cacheblend-arxiv24,
	author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
	journal = {arXiv preprint arXiv:2405.16444},
	title = {{CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion}},
	year = {2024}}

@article{siren-arxiv23,
	author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
	journal = {arXiv preprint arXiv:2309.01219},
	title = {{Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}},
	year = {2023}}

@inproceedings{rag-nips20,
	author = {Yih, Scott},
	booktitle = {Conference on Neural Information Processing Systems, Vancouver, Canada},
	title = {{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}},
	year = {2020}}

@inproceedings{calibrate-icml21,
	author = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	month = {18--24 Jul},
	pages = {12697--12706},
	series = {Proceedings of Machine Learning Research},
	title = {{Calibrate Before Use: Improving Few-shot Performance of Language Models}},
	volume = {139},
	year = {2021}}

@inproceedings{selfcons-ase23,
	author = {Ahmed, Toufique and Devanbu, Premkumar},
	booktitle = {2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
	date-modified = {2024-09-16 07:27:42 +0800},
	organization = {IEEE},
	pages = {1742--1746},
	title = {Better Patching Using LLM Prompting, via Self-Consistency},
	year = {2023}}

@misc{chameleon-nips23,
	archiveprefix = {arXiv},
	author = {Pan Lu and Baolin Peng and Hao Cheng and Michel Galley and Kai-Wei Chang and Ying Nian Wu and Song-Chun Zhu and Jianfeng Gao},
	eprint = {2304.09842},
	primaryclass = {cs.CL},
	title = {Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},
	url = {https://arxiv.org/abs/2304.09842},
	year = {2023},
	bdsk-url-1 = {https://arxiv.org/abs/2304.09842}}

@misc{chameleon-prompt,
	author = {Pan Lu},
	date-modified = {2024-09-16 07:29:21 +0800},
	note = {\url{https://github.com/lupantech/chameleon-llm/blob/main/run_tabmwp/demos/prompt_policy.py}},
	title = {Chameleon-LLM},
	year = {2024}}

@misc{sharegpt,
	author = {ShareGPT teams},
	note = {\url{https://sharegpt.com}},
	title = {ShareGPT},
	year = {2023}}

@article{notalllayers-arxiv24,
	author = {Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan},
	journal = {arXiv preprint arXiv:2403.02181},
	title = {Not all layers of llms are necessary during inference},
	year = {2024}}

@article{alluneed-nips17,
	author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	date-modified = {2024-09-16 07:23:30 +0800},
	journal = {Advances in Neural Information Processing Systems},
	title = {Attention Is All You Need},
	year = {2017}}

@inproceedings{infinigen-osdi24,
	author = {Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
	bdsk-color = {1},
	booktitle = {Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
	date-modified = {2024-09-16 07:38:31 +0800},
	pages = {155--172},
	title = {InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management},
	year = {2024}}

@article{h2o-nips23,
	author = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
	journal = {Advances in Neural Information Processing Systems},
	title = {H2o: Heavy-hitter oracle for efficient generative inference of large language models},
	volume = {36},
	year = {2023}}

@inproceedings{flexgen-icml23,
	author = {Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {31094--31116},
	title = {Flexgen: High-throughput generative inference of large language models with a single gpu},
	year = {2023}}

@article{scaling-mlsys23,
	author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
	journal = {Proceedings of Machine Learning and Systems},
	pages = {606--624},
	title = {Efficiently scaling transformer inference},
	volume = {5},
	year = {2023}}

@article{scissorhands-nips23,
	author = {Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
	journal = {Advances in Neural Information Processing Systems},
	title = {Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
	volume = {36},
	year = {2023}}

@article{streamingllm-arxiv23,
	author = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
	journal = {arXiv preprint arXiv:2309.17453},
	title = {Efficient streaming language models with attention sinks},
	year = {2023}}

@article{kvquant-arxiv24,
	author = {Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
	journal = {arXiv preprint arXiv:2401.18079},
	title = {Kvquant: Towards 10 million context length llm inference with kv cache quantization},
	year = {2024}}

@article{kivi-arxiv24,
	author = {Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
	journal = {arXiv preprint arXiv:2402.02750},
	title = {Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
	year = {2024}}

@article{wkvquant-arxiv24,
	author = {Yue, Yuxuan and Yuan, Zhihang and Duanmu, Haojie and Zhou, Sifan and Wu, Jianlong and Nie, Liqiang},
	journal = {arXiv preprint arXiv:2402.12065},
	title = {Wkvquant: Quantizing weight and key/value cache for large language models gains more},
	year = {2024}}

@inproceedings{taming-osdi24,
	author = {Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav and Tumanov, Alexey and Ramjee, Ramachandran},
	bdsk-color = {1},
	booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
	date-modified = {2024-09-16 07:40:44 +0800},
	pages = {117--134},
	title = {Taming Throughput-Latency Tradeoff in LLM Inference With Sarathi-Serve},
	year = {2024}}

@inproceedings{orca-osdi22,
	author = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
	bdsk-color = {1},
	booktitle = {proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
	date-modified = {2024-09-16 07:39:49 +0800},
	pages = {521--538},
	title = {Orca: A Distributed Serving System for Transformer-Based Generative Models},
	year = {2022}}

@inproceedings{distserve-osdi24,
	author = {Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
	bdsk-color = {1},
	booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
	date-modified = {2024-09-16 07:37:36 +0800},
	pages = {193--210},
	title = {DistServe: Disaggregating Prefill and Decoding for Goodput-Optimized Large Language Model Serving},
	year = {2024}}

@article{pdserve-arxiv24,
	author = {Jin, Yibo and Wang, Tao and Lin, Huimin and Song, Mingyang and Li, Peiyang and Ma, Yipeng and Shan, Yicheng and Yuan, Zhengfan and Li, Cailong and Sun, Yajing and others},
	journal = {arXiv preprint arXiv:2408.08147},
	title = {P/D-Serve: Serving Disaggregated Large Language Model at Scale},
	year = {2024}}

@inproceedings{splitwise-isca24,
	author = {Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
	booktitle = {2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
	organization = {IEEE},
	pages = {118--132},
	title = {Splitwise: Efficient generative llm inference using phase splitting},
	year = {2024}}

@article{tetriinf-arxiv24,
	author = {Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others},
	journal = {arXiv preprint arXiv:2401.11181},
	title = {Inference without interference: Disaggregate llm inference for mixed downstream workloads},
	year = {2024}}

@article{dv-arxiv24,
	author = {Strati, Foteini and Mcallister, Sara and Phanishayee, Amar and Tarnawski, Jakub and Klimovic, Ana},
	bdsk-color = {1},
	date-modified = {2024-09-16 07:45:01 +0800},
	journal = {arXiv preprint arXiv:2403.01876},
	title = {D\'ej\`aVu: KV-Cache Streaming for Fast, Fault-Tolerant Generative LLM Serving},
	year = {2024}}

@inproceedings{alpaserve-osdi23,
	author = {Li, Zhuohan and Zheng, Lianmin and Zhong, Yinmin and Liu, Vincent and Sheng, Ying and Jin, Xin and Huang, Yanping and Chen, Zhifeng and Zhang, Hao and Gonzalez, Joseph E and others},
	bdsk-color = {1},
	booktitle = {Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
	date-modified = {2024-09-16 07:19:16 +0800},
	pages = {663--679},
	title = {AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
	year = {2023}}

@article{loongserve-arxiv24,
	author = {Wu, Bingyang and Liu, Shengyu and Zhong, Yinmin and Sun, Peng and Liu, Xuanzhe and Jin, Xin},
	journal = {arXiv preprint arXiv:2404.09526},
	title = {LoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism},
	year = {2024}}

@inproceedings{vllm-sosp23,
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
	booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
	pages = {611--626},
	title = {Efficient memory management for large language model serving with pagedattention},
	year = {2023}}

@article{infinite-arxiv24,
	author = {Lin, Bin and Peng, Tao and Zhang, Chen and Sun, Minmin and Li, Lanbo and Zhao, Hanyu and Xiao, Wencong and Xu, Qi and Qiu, Xiafei and Li, Shen and others},
	journal = {arXiv preprint arXiv:2401.02669},
	title = {Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache},
	year = {2024}}

@article{mooncake-arxiv24,
	author = {Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
	journal = {arXiv preprint arXiv:2407.00079},
	title = {Mooncake: Kimi's KVCache-centric Architecture for LLM Serving},
	year = {2024}}

@article{chatgpt-23,
	author = {Wu, Tianyu and He, Shizhu and Liu, Jingping and Sun, Siqi and Liu, Kang and Han, Qing-Long and Tang, Yang},
	date-modified = {2024-09-16 07:16:14 +0800},
	journal = {IEEE/CAA Journal of Automatica Sinica},
	number = {5},
	pages = {1122--1136},
	publisher = {IEEE},
	title = {A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development},
	volume = {10},
	year = {2023}}

@article{chatbotmed-23,
	author = {Lee, Peter and Bubeck, Sebastien and Petro, Joseph},
	date-modified = {2024-09-16 07:27:10 +0800},
	journal = {New England Journal of Medicine},
	number = {13},
	pages = {1233--1239},
	publisher = {Mass Medical Soc},
	title = {Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine},
	volume = {388},
	year = {2023}}

@article{summarization-23,
	author = {Bharathi Mohan, G and Prasanna Kumar, R and Parathasarathy, Srinivasan and Aravind, S and Hanish, KB and Pavithria, G},
	journal = {Data Analytics for Internet of Things Infrastructure},
	pages = {247--264},
	publisher = {Springer},
	title = {Text summarization for big data analytics: a comprehensive review of GPT 2 and BERT approaches},
	year = {2023}}

@article{summarization2-22,
	author = {Ghadimi, Alireza and Beigy, Hamid},
	journal = {Expert Systems with Applications},
	pages = {116292},
	publisher = {Elsevier},
	title = {Hybrid multi-document summarization using pre-trained language models},
	volume = {192},
	year = {2022}}

@inproceedings{gpt4trans-23,
	author = {Raunak, Vikas and	Sharaf, Amr and Wang, Yiren and Awadalla, Hany and Menezes, Arul},
	booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
	pages = {12009--12024},
	title = {Leveraging {GPT}-4 for Automatic Translation Post-Editing},
	year = {2023}}

@article{goodgpttrans-23,
	author = {Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
	journal = {arXiv preprint arXiv:2302.09210},
	title = {How good are gpt models at machine translation? a comprehensive evaluation},
	year = {2023}}

@article{jaccard-18,
	author = {Fletcher, Sam and Islam, Md Zahidul and others},
	date-modified = {2024-09-16 07:31:57 +0800},
	journal = {Australasian Journal of Information Systems},
	publisher = {Australian Computer Society},
	title = {Comparing Sets of Patterns with the Jaccard Index},
	volume = {22},
	year = {2018}}

@article{opt-arxiv22,
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
	journal = {arXiv preprint arXiv:2205.01068},
	title = {Opt: Open pre-trained transformer language models},
	year = {2022}}

@article{lmeval,
	author = {Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
	date-modified = {2024-09-16 07:17:33 +0800},
	journal = {Version v0. 0.1. Sept},
	pages = {8--9},
	title = {A Framework for Few-Shot Language Model Evaluation},
	volume = {10},
	year = {2021}}

@article{squad-arxiv18,
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	journal = {arXiv preprint arXiv:1806.03822},
	title = {Know what you don't know: Unanswerable questions for SQuAD},
	year = {2018}}

@misc{gptsysprompt,
	author = {Vlad Alex},
	note = {\url{https://github.com/mustvlad/ChatGPT-System-Prompts}},
	title = {{ChatGPT System Prompts}},
	year = {2024}}

@inproceedings{attacc-asplos24,
	author = {Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho},
	booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
	date-modified = {2024-09-16 07:20:07 +0800},
	pages = {103--119},
	title = {AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference},
	year = {2024}}
