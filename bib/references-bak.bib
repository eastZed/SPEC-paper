@article{chunkattention-arxiv24,
  title={{Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition}},
  author={Ye, Lu and Tao, Ze and Huang, Yong and Li, Yang},
  journal={arXiv preprint arXiv:2402.15220},
  year={2024}
}

@article{hydragen-arxiv24,
  title={Hydragen: High-Throughput LLM Inference with Shared Prefixes},
  author={Juravsky, Jordan and Brown, Bradley and Ehrlich, Ryan and Fu, Daniel Y and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2402.05099},
  year={2024}
}

@article{promptcache-mlsys24,
  title={Prompt cache: Modular attention reuse for low-latency inference},
  author={Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={325--338},
  year={2024}
}

@article{sglang-arxiv23,
  title={Efficiently programming large language models using sglang},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Huang, Jeff and Sun, Chuyue and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2312.07104},
  year={2023}
}

@article{attentionstore-atc24,
  title={AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving},
  author={Gao, Bin and He, Zhuomin and Sharma, Puru and Kang, Qingxuan and Jevdjic, Djordje and Deng, Junbo and Yang, Xingkun and Yu, Zhou and Zuo, Pengfei},
  journal={arXiv preprint arXiv:2403.19708},
  year={2024}
}

@article{ragcache-arxiv24,
  title={RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation},
  author={Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2404.12457},
  year={2024}
}

@inproceedings{cachegen-sigcomm24,
  title={CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{cacheblend-arxiv24,
  title={CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion},
  author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  journal={arXiv preprint arXiv:2405.16444},
  year={2024}
}

@article{siren-arxiv23,
	title={Siren's song in the AI ocean: a survey on hallucination in large language models},
	author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
	journal={arXiv preprint arXiv:2309.01219},
	year={2023}
}

@inproceedings{rag-nips20,
	title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
	author={Yih, Scott},
	booktitle={Conference on Neural Information Processing Systems, Vancouver, Canada},
	year={2020}
}

@inproceedings{calibrate-icml21,
	title={Calibrate Before Use: Improving Few-shot Performance of Language Models},
	author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
	booktitle={Proceedings of the 38th International Conference on Machine Learning},
	pages={12697--12706},
	year={2021},
	volume={139},
	series={Proceedings of Machine Learning Research},
	month={18--24 Jul},
}

@inproceedings{selfcons-ase23,
	title={Better patching using LLM prompting, via Self-Consistency},
	author={Ahmed, Toufique and Devanbu, Premkumar},
	booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
	pages={1742--1746},
	year={2023},
	organization={IEEE}
}

@misc{chameleon-nips23,
	title={Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models}, 
	author={Pan Lu and Baolin Peng and Hao Cheng and Michel Galley and Kai-Wei Chang and Ying Nian Wu and Song-Chun Zhu and Jianfeng Gao},
	year={2023},
	eprint={2304.09842},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2304.09842}, 
}

@misc{chameleon-prompt,
	author = {Pan Lu},
	title = {chameleon-llm},  note = {\url{https://github.com/lupantech/chameleon-llm/blob/main/run_tabmwp/demos/prompt_policy.py}}, 
	year={2024},
}

@misc{sharegpt,
	author={ShareGPT teams},
	title="ShareGPT",
	note = {\url{https://sharegpt.com}},
	year={2023},
}

@article{notalllayers-arxiv24,
	title={Not all layers of llms are necessary during inference},
	author={Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan},
	journal={arXiv preprint arXiv:2403.02181},
	year={2024}
}

@article{alluneed-nips17,
	title={Attention is all you need},
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	journal={Advances in Neural Information Processing Systems},
	year={2017}
}

@inproceedings{infinigen-osdi24,
	title={$\{$InfiniGen$\}$: Efficient Generative Inference of Large Language Models with Dynamic $\{$KV$\}$ Cache Management},
	author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
	booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
	pages={155--172},
	year={2024}
}

@article{h2o-nips23,
	title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
	author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2023}
}

@inproceedings{flexgen-icml23,
	title={Flexgen: High-throughput generative inference of large language models with a single gpu},
	author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
	booktitle={International Conference on Machine Learning},
	pages={31094--31116},
	year={2023},
	organization={PMLR}
}

@article{scaling-mlsys23,
	title={Efficiently scaling transformer inference},
	author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
	journal={Proceedings of Machine Learning and Systems},
	volume={5},
	pages={606--624},
	year={2023}
}

@article{scissorhands-nips23,
	title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
	author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2023}
}

@article{streamingllm-arxiv23,
	title={Efficient streaming language models with attention sinks},
	author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
	journal={arXiv preprint arXiv:2309.17453},
	year={2023}
}

@article{kvquant-arxiv24,
	title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
	author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
	journal={arXiv preprint arXiv:2401.18079},
	year={2024}
}

@article{kivi-arxiv24,
	title={Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
	author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
	journal={arXiv preprint arXiv:2402.02750},
	year={2024}
}

@article{wkvquant-arxiv24,
	title={Wkvquant: Quantizing weight and key/value cache for large language models gains more},
	author={Yue, Yuxuan and Yuan, Zhihang and Duanmu, Haojie and Zhou, Sifan and Wu, Jianlong and Nie, Liqiang},
	journal={arXiv preprint arXiv:2402.12065},
	year={2024}
}

@inproceedings{taming-osdi24,
	title={Taming $\{$Throughput-Latency$\}$ Tradeoff in $\{$LLM$\}$ Inference with $\{$Sarathi-Serve$\}$},
	author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav and Tumanov, Alexey and Ramjee, Ramachandran},
	booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
	pages={117--134},
	year={2024}
}

@inproceedings{orca-osdi22,
	title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
	author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
	booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
	pages={521--538},
	year={2022}
}

@inproceedings{distserve-osdi24,
	title={$\{$DistServe$\}$: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving},
	author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
	booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
	pages={193--210},
	year={2024}
}

@article{pdserve-arxiv24,
	title={P/D-Serve: Serving Disaggregated Large Language Model at Scale},
	author={Jin, Yibo and Wang, Tao and Lin, Huimin and Song, Mingyang and Li, Peiyang and Ma, Yipeng and Shan, Yicheng and Yuan, Zhengfan and Li, Cailong and Sun, Yajing and others},
	journal={arXiv preprint arXiv:2408.08147},
	year={2024}
}

@inproceedings{splitwise-isca24,
	title={Splitwise: Efficient generative llm inference using phase splitting},
	author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
	booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
	pages={118--132},
	year={2024},
	organization={IEEE}
}

@article{tetriinf-arxiv24,
	title={Inference without interference: Disaggregate llm inference for mixed downstream workloads},
	author={Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others},
	journal={arXiv preprint arXiv:2401.11181},
	year={2024}
}

@article{dv-arxiv24,
	title={D$\backslash$'ej$\backslash$aVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving},
	author={Strati, Foteini and Mcallister, Sara and Phanishayee, Amar and Tarnawski, Jakub and Klimovic, Ana},
	journal={arXiv preprint arXiv:2403.01876},
	year={2024}
}

@inproceedings{alpaserve-osdi23,
	title={$\{$AlpaServe$\}$: Statistical multiplexing with model parallelism for deep learning serving},
	author={Li, Zhuohan and Zheng, Lianmin and Zhong, Yinmin and Liu, Vincent and Sheng, Ying and Jin, Xin and Huang, Yanping and Chen, Zhifeng and Zhang, Hao and Gonzalez, Joseph E and others},
	booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
	pages={663--679},
	year={2023}
}

@article{loongserve-arxiv24,
	title={LoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism},
	author={Wu, Bingyang and Liu, Shengyu and Zhong, Yinmin and Sun, Peng and Liu, Xuanzhe and Jin, Xin},
	journal={arXiv preprint arXiv:2404.09526},
	year={2024}
}

@inproceedings{vllm-sosp23,
	title={Efficient memory management for large language model serving with pagedattention},
	author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
	booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
	pages={611--626},
	year={2023}
}

@article{infinite-arxiv24,
	title={Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache},
	author={Lin, Bin and Peng, Tao and Zhang, Chen and Sun, Minmin and Li, Lanbo and Zhao, Hanyu and Xiao, Wencong and Xu, Qi and Qiu, Xiafei and Li, Shen and others},
	journal={arXiv preprint arXiv:2401.02669},
	year={2024}
}

@article{mooncake-arxiv24,
	title={Mooncake: Kimi's KVCache-centric Architecture for LLM Serving},
	author={Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
	journal={arXiv preprint arXiv:2407.00079},
	year={2024}
}

@article{chatgpt-23,
	title={A brief overview of ChatGPT: The history, status quo and potential future development},
	author={Wu, Tianyu and He, Shizhu and Liu, Jingping and Sun, Siqi and Liu, Kang and Han, Qing-Long and Tang, Yang},
	journal={IEEE/CAA Journal of Automatica Sinica},
	volume={10},
	number={5},
	pages={1122--1136},
	year={2023},
	publisher={IEEE}
}

@article{chatbotmed-23,
	title={Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine},
	author={Lee, Peter and Bubeck, Sebastien and Petro, Joseph},
	journal={New England Journal of Medicine},
	volume={388},
	number={13},
	pages={1233--1239},
	year={2023},
	publisher={Mass Medical Soc}
}

@article{summarization-23,
	title={Text summarization for big data analytics: a comprehensive review of GPT 2 and BERT approaches},
	author={Bharathi Mohan, G and Prasanna Kumar, R and Parathasarathy, Srinivasan and Aravind, S and Hanish, KB and Pavithria, G},
	journal={Data Analytics for Internet of Things Infrastructure},
	pages={247--264},
	year={2023},
	publisher={Springer}
}

@article{summarization2-22,
	title={Hybrid multi-document summarization using pre-trained language models},
	author={Ghadimi, Alireza and Beigy, Hamid},
	journal={Expert Systems with Applications},
	volume={192},
	pages={116292},
	year={2022},
	publisher={Elsevier}
}

@inproceedings{gpt4trans-23,
	title={Leveraging {GPT}-4 for Automatic Translation Post-Editing},
	author={Raunak, Vikas  and	Sharaf, Amr  and Wang, Yiren  and Awadalla, Hany  and Menezes, Arul},
	booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
	year={2023},
	pages={12009--12024},
}

@article{goodgpttrans-23,
	title={How good are gpt models at machine translation? a comprehensive evaluation},
	author={Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
	journal={arXiv preprint arXiv:2302.09210},
	year={2023}
}

@article{jaccard-18,
	title={Comparing sets of patterns with the Jaccard index},
	author={Fletcher, Sam and Islam, Md Zahidul and others},
	journal={Australasian Journal of Information Systems},
	volume={22},
	year={2018},
	publisher={Australian Computer Society}
}

@article{opt-arxiv22,
	title={Opt: Open pre-trained transformer language models},
	author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
	journal={arXiv preprint arXiv:2205.01068},
	year={2022}
}

@article{lmeval,
	title={A framework for few-shot language model evaluation},
	author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
	journal={Version v0. 0.1. Sept},
	volume={10},
	pages={8--9},
	year={2021}
}

@article{squad-arxiv18,
	title={Know what you don't know: Unanswerable questions for SQuAD},
	author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	journal={arXiv preprint arXiv:1806.03822},
	year={2018}
}

@misc{gptsysprompt,
	author = {Vlad Alex},
	title = {{ChatGPT System Prompts}},  
	note = {\url{https://github.com/mustvlad/ChatGPT-System-Prompts}}, 
	year={2024},
}

@inproceedings{attacc-asplos24,
  title={AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference},
  author={Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={103--119},
  year={2024}
}

