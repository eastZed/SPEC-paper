\begin{thebibliography}{10}

\bibitem{taming-osdi24}
Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra,
  Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee.
\newblock {Taming Throughput-Latency Tradeoff in LLM Inference with
  Sarathi-Serve}.
\newblock In {\em Proceedings of the 18th USENIX Symposium on Operating Systems
  Design and Implementation (OSDI)}, pages 117--134, 2024.

\bibitem{selfcons-ase23}
Toufique Ahmed and Premkumar Devanbu.
\newblock {Better Patching Using LLM Prompting, via Self-Consistency}.
\newblock In {\em Proceedings of the 38th IEEE/ACM International Conference on
  Automated Software Engineering (ASE)}, pages 1742--1746. IEEE, 2023.

\bibitem{summarization-23}
G~Bharathi~Mohan, R~Prasanna~Kumar, Srinivasan Parathasarathy, S~Aravind,
  KB~Hanish, and G~Pavithria.
\newblock {Text Summarization for Big Data Analytics: A Comprehensive Review of
  GPT 2 and BERT Approaches}.
\newblock {\em Data Analytics for Internet of Things Infrastructure}, pages
  247--264, 2023.

\bibitem{impress-fast25}
Weijian Chen, Shuibing He, Haoyang Qu, Ruidong Zhang, Siling Yang, Ping Chen,
  Yi~Zheng, Baoxing Huai, and Gang Chen.
\newblock {IMPRESS}: An {Importance-Informed} {Multi-Tier} prefix {KV} storage
  system for large language model inference.
\newblock In {\em 23rd USENIX Conference on File and Storage Technologies (FAST
  25)}, pages 187--201, Santa Clara, CA, February 2025. USENIX Association.

\bibitem{jaccard-18}
Sam Fletcher, Md~Zahidul Islam, et~al.
\newblock {Comparing Sets of Patterns with the Jaccard Index}.
\newblock {\em Australasian Journal of Information Systems}, 22, 2018.

\bibitem{attentionstore-atc24}
Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng,
  Xingkun Yang, Zhou Yu, and Pengfei Zuo.
\newblock {AttentionStore: Cost-Effective Attention Reuse across Multi-Turn
  Conversations in Large Language Model Serving}.
\newblock {\em arXiv preprint arXiv:2403.19708}, 2024.

\bibitem{lmeval}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  et~al.
\newblock {A Framework for Few-Shot Language Model Evaluation}, 2024.
\newblock \url{https://zenodo.org/records/12608602}.

\bibitem{summarization2-22}
Alireza Ghadimi and Hamid Beigy.
\newblock {Hybrid Multi-Document Summarization using Pre-Trained Language
  Models}.
\newblock {\em Expert Systems with Applications}, 192:116292, 2022.

\bibitem{promptcache-mlsys24}
In~Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin
  Zhong.
\newblock {Prompt Cache: Modular Attention Reuse for Low-Latency Inference}.
\newblock In {\em Proceedings of Machine Learning and Systems (MLSys)},
  volume~6, pages 325--338, 2024.

\bibitem{goodgpttrans-23}
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu
  Matsushita, Young~Jin Kim, Mohamed Afify, and Hany~Hassan Awadalla.
\newblock {How Good are GPT Models at Machine Translation? A Comprehensive
  Evaluation}.
\newblock {\em arXiv preprint arXiv:2302.09210}, 2023.

\bibitem{kvquant-arxiv24}
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael~W Mahoney, Yakun~Sophia
  Shao, Kurt Keutzer, and Amir Gholami.
\newblock {Kvquant: Towards 10 Million Context Length LLM Inference with KV
  Cache Quantization}.
\newblock {\em arXiv preprint arXiv:2401.18079}, 2024.

\bibitem{tetriinf-arxiv24}
Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen,
  Hao Feng, Chenxi Wang, Sa~Wang, Yungang Bao, et~al.
\newblock {Inference without Interference: Disaggregate LLM Inference for Mixed
  Downstream Workloads}.
\newblock {\em arXiv preprint arXiv:2401.11181}, 2024.

\bibitem{ragcache-arxiv24}
Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin
  Jin.
\newblock {RAGCache: Efficient Knowledge Caching for Retrieval-Augmented
  Generation}.
\newblock {\em arXiv preprint arXiv:2404.12457}, 2024.

\bibitem{pdserve-arxiv24}
Yibo Jin, Tao Wang, Huimin Lin, Mingyang Song, Peiyang Li, Yipeng Ma, Yicheng
  Shan, Zhengfan Yuan, Cailong Li, Yajing Sun, et~al.
\newblock {P/D-Serve: Serving Disaggregated Large Language Model at Scale}.
\newblock {\em arXiv preprint arXiv:2408.08147}, 2024.

\bibitem{hydragen-arxiv24}
Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel~Y Fu, Christopher R{\'e},
  and Azalia Mirhoseini.
\newblock {Hydragen: High-Throughput LLM Inference with Shared Prefixes}.
\newblock {\em arXiv preprint arXiv:2402.05099}, 2024.

\bibitem{vllm-sosp23}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu,
  Joseph Gonzalez, Hao Zhang, and Ion Stoica.
\newblock {Efficient Memory Management for Large Language Model Serving with
  PagedAttention}.
\newblock In {\em Proceedings of the 29th Symposium on Operating Systems
  Principles (SOSP)}, pages 611--626, 2023.

\bibitem{chatbotmed-23}
Peter Lee, Sebastien Bubeck, and Joseph Petro.
\newblock {Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine}.
\newblock {\em New England Journal of Medicine}, 388(13):1233--1239, 2023.

\bibitem{infinigen-osdi24}
Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim.
\newblock {InfiniGen: Efficient Generative Inference of Large Language Models
  with Dynamic KV Cache Management}.
\newblock In {\em Proceedings of the 18th USENIX Symposium on Operating Systems
  Design and Implementation (OSDI)}, pages 155--172, 2024.

\bibitem{rag-nips20}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K\"{u}ttler, Mike Lewis, Wen-tau Yih, Tim
  Rockt\"{a}schel, Sebastian Riedel, and Douwe Kiela.
\newblock {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}.
\newblock In {\em Proceedings of the Conference on Neural Information
  Processing Systems (NeurIPS)}, volume~33, pages 9459--9474, 2020.

\bibitem{alpaserve-osdi23}
Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin,
  Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph~E Gonzalez, et~al.
\newblock {AlpaServe: Statistical Multiplexing with Model Parallelism for Deep
  Learning Serving}.
\newblock In {\em Proceedings of the 17th USENIX Symposium on Operating Systems
  Design and Implementation (OSDI)}, pages 663--679, 2023.

\bibitem{infinite-arxiv24}
Bin Lin, Tao Peng, Chen Zhang, Minmin Sun, Lanbo Li, Hanyu Zhao, Wencong Xiao,
  Qi~Xu, Xiafei Qiu, Shen Li, et~al.
\newblock {Infinite-LLM: Efficient LLM Service for Long Context with
  DistAttention and Distributed KVCache}.
\newblock {\em arXiv preprint arXiv:2401.02669}, 2024.

\bibitem{cachegen-sigcomm24}
Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang,
  Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, et~al.
\newblock {CacheGen: KV Cache Compression and Streaming for Fast Large Language
  Model Serving}.
\newblock In {\em Proceedings of the ACM Special Interest Group on Data
  Communication Conference (SIGCOMM)}, pages 38--56, 2024.

\bibitem{scissorhands-nips23}
Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu,
  Anastasios Kyrillidis, and Anshumali Shrivastava.
\newblock {Scissorhands: Exploiting the Persistence of Importance Hypothesis
  for LLM KV Cache Compression at Test Time}.
\newblock In {\em Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, volume~36, pages 52342--52364, 2023.

\bibitem{kivi-arxiv24}
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir
  Braverman, Beidi Chen, and Xia Hu.
\newblock {KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache}.
\newblock {\em arXiv preprint arXiv:2402.02750}, 2024.

\bibitem{chameleon-prompt}
Pan Lu.
\newblock {Chameleon-LLM}, 2024.
\newblock
  \url{https://github.com/lupantech/chameleon-llm/blob/main/run_tabmwp/demos/prompt_policy.py}.

\bibitem{chameleon-nips23}
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying~Nian Wu,
  Song-Chun Zhu, and Jianfeng Gao.
\newblock {Chameleon: Plug-and-Play Compositional Reasoning with Large Language
  Models}.
\newblock In {\em Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, volume~36, pages 43447--43478, 2023.

\bibitem{attacc-asplos24}
Jaehyun Park, Jaewan Choi, Kwanhee Kyung, Michael~Jaemin Kim, Yongsuk Kwon,
  Nam~Sung Kim, and Jung~Ho Ahn.
\newblock {AttAcc! Unleashing the Power of PIM for Batched Transformer-based
  Generative Model Inference}.
\newblock In {\em Proceedings of the 29th ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems
  (ASPLOS)}, volume~2, pages 103--119, 2024.

\bibitem{splitwise-isca24}
Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, {\'I}{\~n}igo Goiri,
  Saeed Maleki, and Ricardo Bianchini.
\newblock {Splitwise: Efficient Generative LLM Inference using Phase
  Splitting}.
\newblock In {\em Proceedings of ACM/IEEE 51st Annual International Symposium
  on Computer Architecture (ISCA)}, pages 118--132, 2024.

\bibitem{scaling-mlsys23}
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
  Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.
\newblock {Efficiently Scaling Transformer Inference}.
\newblock In {\em Proceedings of Machine Learning and Systems (MLSys)},
  volume~5, pages 606--624, 2023.

\bibitem{mooncake-arxiv24}
Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and
  Xinran Xu.
\newblock {Mooncake: A KVCache-Centric Disaggregated Architecture for LLM
  Serving}.
\newblock {\em arXiv preprint arXiv:2407.00079}, 2024.

\bibitem{squad-arxiv18}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock {Know What You Don't Know: Unanswerable Questions for SQuAD}.
\newblock {\em arXiv preprint arXiv:1806.03822}, 2018.

\bibitem{gpt4trans-23}
Vikas Raunak, Amr Sharaf, Yiren Wang, Hany Awadalla, and Arul Menezes.
\newblock {Leveraging {GPT}-4 for Automatic Translation Post-Editing}.
\newblock In {\em Proceedings of Findings of the Association for Computational
  Linguistics (EMNLP)}, pages 12009--12024, 2023.

\bibitem{flexgen-icml23}
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen,
  Percy Liang, Christopher R{\'e}, Ion Stoica, and Ce~Zhang.
\newblock {FlexGen: High-Throughput Generative Inference of Large Language
  Models with A Single GPU}.
\newblock In {\em Proceedings of International Conference on Machine Learning
  (ICML)}, pages 31094--31116, 2023.

\bibitem{dv-arxiv24}
Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub Tarnawski, and Ana
  Klimovic.
\newblock {D\'ej\`aVu: KV-Cache Streaming for Fast, Fault-Tolerant Generative
  LLM Serving}.
\newblock {\em arXiv preprint arXiv:2403.01876}, 2024.

\bibitem{sharegpt}
ShareGPT Teams.
\newblock {ShareGPT}, 2023.
\newblock \url{https://sharegpt.com}.

\bibitem{alluneed-nips17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock {Attention Is All You Need}.
\newblock In {\em Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, volume~30, 2017.

\bibitem{loongserve-arxiv24}
Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, and Xin Jin.
\newblock {LoongServe: Efficiently Serving Long-context Large Language Models
  with Elastic Sequence Parallelism}.
\newblock {\em arXiv preprint arXiv:2404.09526}, 2024.

\bibitem{chatgpt-23}
Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and Yang
  Tang.
\newblock {A Brief Overview of ChatGPT: The History, Status Quo and Potential
  Future Development}.
\newblock {\em IEEE/CAA Journal of Automatica Sinica}, 10(5):1122--1136, 2023.

\bibitem{cacheblend-arxiv24}
Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang,
  Kuntai Du, Shan Lu, and Junchen Jiang.
\newblock {CacheBlend: Fast Large Language Model Serving with Cached Knowledge
  Fusion}.
\newblock {\em arXiv preprint arXiv:2405.16444}, 2024.

\bibitem{chunkattention-arxiv24}
Lu~Ye, Ze~Tao, Yong Huang, and Yang Li.
\newblock {Chunkattention: Efficient Self-Attention With Prefix-Aware KV Cache
  and Two-Phase Partition}.
\newblock {\em arXiv preprint arXiv:2402.15220}, 2024.

\bibitem{orca-osdi22}
Gyeong-In Yu, Joo~Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.
\newblock {Orca: A Distributed Serving System for Transformer-Based Generative
  Models}.
\newblock In {\em Proceedings of the 16th USENIX Symposium on Operating Systems
  Design and Implementation (OSDI)}, pages 521--538, 2022.

\bibitem{wkvquant-arxiv24}
Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, and Liqiang
  Nie.
\newblock {WKVQuant: Quantizing Weight and Key/Value Cache for Large Language
  Models Gains More}.
\newblock {\em arXiv preprint arXiv:2402.12065}, 2024.

\bibitem{opt-arxiv22}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock {OPT: Open Pre-Trained Transformer Language Models}.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{siren-arxiv23}
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
  Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, et~al.
\newblock {Siren's Song in the AI Ocean: A Survey on Hallucination in Large
  Language Models}.
\newblock {\em arXiv preprint arXiv:2309.01219}, 2023.

\bibitem{h2o-nips23}
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai,
  Zhao Song, Yuandong Tian, Christopher R{\'e}, Clark Barrett, et~al.
\newblock {H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large
  Language Models}.
\newblock In {\em Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, volume~36, pages 34661--34710, 2023.

\bibitem{sglang-arxiv23}
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody~Hao
  Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph~E Gonzalez, et~al.
\newblock {SGLang: Efficient Execution of Structured Language Model Programs}.
\newblock {\em arXiv preprint arXiv:2312.07104}, 2023.

\bibitem{distserve-osdi24}
Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin
  Jin, and Hao Zhang.
\newblock {DistServe: Disaggregating Prefill and Decoding for Goodput-Optimized
  Large Language Model Serving}.
\newblock In {\em Proceedings of the 18th USENIX Symposium on Operating Systems
  Design and Implementation (OSDI)}, pages 193--210, 2024.

\end{thebibliography}
